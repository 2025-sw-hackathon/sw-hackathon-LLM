#!/usr/bin/env python
# core/model.py
import torch
from typing import Tuple, List
from unsloth import FastLanguageModel
from config.settings import MODEL_ID, DEVICE

def load_llm() -> Tuple:
    """Unsloth LLM ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    print(f"ğŸ”„ Unsloth Gemma-3 ({MODEL_ID}) ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name=MODEL_ID,
        max_seq_length=4096,
        dtype=None,
        load_in_4bit=True,
    )
    return model, tokenizer

def prepare_prompt_template() -> str:
    """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    system_tmpl = """
ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Gemma-3 ëª¨ë¸ ê¸°ë°˜ì˜ ëŒ€í•™ ì •ë³´ ì•ˆë‚´ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤ ğŸ˜Š  
ì•„ë˜ <ë¬¸ì„œ>ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ë”°ëœ»í•˜ê²Œ,  
ê·¸ë¦¬ê³  **ìµœì†Œ 5ë¬¸ì¥ ì´ìƒ**ìœ¼ë¡œ ìì„¸íˆ ë‹µë³€í•´ë“œë¦´ê²Œìš”.  

<ë¬¸ì„œ>
{context}

<ì§ˆë¬¸>
{question}

<ë‹µë³€>
"""
    return system_tmpl

def generate_response(model, tokenizer, context: str, question: str, history_str: str = "") -> str:
    """ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = prepare_prompt_template().format(context=context, question=question)
    
    # ì „ì²´ í”„ë¡¬í”„íŠ¸
    prompt = system_prompt + history_str
    
    # í† í¬ë‚˜ì´ì¦ˆ & ìƒì„±
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=1024,
        do_sample=False,
        temperature=1,
        top_p=1,
        top_k=1,
    )
    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # prompt ë¶€ë¶„ ì œê±°
    if "<|assistant|>" in resp:
        answer = resp.split("<|assistant|>")[-1].strip()
    elif "<ë‹µë³€>" in resp:
        answer = resp.split("<ë‹µë³€>")[-1].strip()
    else:
        answer = resp.strip()
    
    return answer

def get_source_documents(docs: List) -> List[str]:
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¡œë¶€í„° ì†ŒìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    source_documents = []
    for doc in docs:
        content = doc.page_content
        header_parts = content.split("\n")[0].split("|")
        if len(header_parts) > 0:
            source_documents.append(header_parts[0])
    
    # ì¤‘ë³µ ì œê±°
    return list(set(source_documents))